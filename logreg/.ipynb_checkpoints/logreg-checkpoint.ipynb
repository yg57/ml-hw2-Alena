{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "This file contains code that helps you get started on \n",
    "logistic regression. You will need to complete the functions \n",
    "in logistic_regressor.py and utils.py in the places indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils\n",
    "import utils\n",
    "from logistic_regressor import LogisticRegressor\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unregularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================ Part 0: Reading data and plotting ==================#\n",
    "\n",
    "data = pd.read_csv('ex1data1.txt')\n",
    "X = np.vstack([data.x1,data.x2]).T\n",
    "y = data.y\n",
    "\n",
    "print('Plotting data with green circle indicating (y=1) examples and red circle indicating (y=0) examples ...')\n",
    "plot_utils.plot_twoclass_data(X,y,'Exam 1 score', 'Exam 2 score',['Not Admitted','Admitted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================ Part 1: Compute cost and gradient ==================#\n",
    "# open logistic_regressor.py and implement loss and gradient of loss \n",
    "\n",
    "# set up the X matrix with the column of ones as intercept\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X.T]).T\n",
    "\n",
    "# set up a logistic regression model\n",
    "\n",
    "log_reg1 = LogisticRegressor()\n",
    "\n",
    "# test the loss and gradient function\n",
    "\n",
    "theta = np.zeros((XX.shape[1],))\n",
    "loss = log_reg1.loss(theta,XX,y)\n",
    "print(\"Loss on all-zeros theta vector (should be around 0.693) = %.4f\"  %loss)\n",
    "grad = log_reg1.grad_loss(theta,XX,y)\n",
    "print(\"Gradient of loss wrt all-zeros theta vector (should be around [-0.1, -12.01, -11.26]) = %s\" %grad)\n",
    "\n",
    "# run fmin on the loss function and gradient \n",
    "\n",
    "theta_opt = log_reg1.train(XX,y,num_iters=1000)\n",
    "\n",
    "# print the theta found\n",
    "print('Theta found by fmin_bfgs: %s' %theta_opt)\n",
    "log_reg1.theta = theta_opt\n",
    "print(\"Final loss = %.4f\" %log_reg1.loss(theta_opt,XX,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================ Part 1: Compute cost and gradient ==================#\n",
    "# open logistic_regressor.py and implement loss and gradient of loss \n",
    "\n",
    "# set up the X matrix with the column of ones as intercept\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X.T]).T\n",
    "\n",
    "# set up a logistic regression model\n",
    "\n",
    "log_reg1 = LogisticRegressor()\n",
    "\n",
    "# test the loss and gradient function\n",
    "\n",
    "theta = np.zeros((XX.shape[1],))\n",
    "loss = log_reg1.loss(theta,XX,y)\n",
    "print(\"Loss on all-zeros theta vector (should be around 0.693) = %.4f\"  %loss)\n",
    "grad = log_reg1.grad_loss(theta,XX,y)\n",
    "print(\"Gradient of loss wrt all-zeros theta vector (should be around [-0.1, -12.01, -11.26]) = %s\" %grad)\n",
    "\n",
    "# run fmin on the loss function and gradient \n",
    "\n",
    "theta_opt = log_reg1.train(XX,y,num_iters=400)\n",
    "\n",
    "# print the theta found\n",
    "print('Theta found by fmin_bfgs: %s' %theta_opt)\n",
    "log_reg1.theta = theta_opt\n",
    "print(\"Final loss = %.4f\" %log_reg1.loss(theta_opt,XX,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the method predict in logistic_regressor.py\n",
    "\n",
    "# TODO: calculate the probability of a student being admitted with score of 45,85\n",
    "#       replace pred_prob = 0 with pred_prob = expression for that probability\n",
    "\n",
    "pred_prob = 0\n",
    "print(\"For a student with 45 on exam 1 and 85 on exam 2, the probability of admission = %.4f\" %pred_prob)\n",
    "\n",
    "# compute accuracy on the training set\n",
    "\n",
    "predy = log_reg1.predict(XX)\n",
    "\n",
    "# TODO: calculate the accuracy of predictions on training set (hint: compare predy and y)\n",
    "\n",
    "accuracy = 0\n",
    "print(\"Accuracy on the training set = %.4f\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision surface\n",
    "\n",
    "plot_utils.plot_decision_boundary(X,y,theta_opt,'Exam 1 score', 'Exam 2 score',['Not Admitted','Admitted'])\n",
    "plt.show()\n",
    "\n",
    "# Compare with sklearn logistic regression\n",
    "# note the parameters fed into the LogisticRegression call\n",
    "\n",
    "from sklearn import linear_model\n",
    "sk_logreg = linear_model.LogisticRegression(C=1e5,solver='lbfgs',fit_intercept=False)\n",
    "sk_logreg.fit(XX,y)\n",
    "print(\"Theta found by sklearn: %s\" %sk_logreg.coef_)\n",
    "\n",
    "plot_utils.plot_decision_boundary_sklearn(X,y,sk_logreg,'Exam 1 score', 'Exam 2 score',['Not Admitted','Admitted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
